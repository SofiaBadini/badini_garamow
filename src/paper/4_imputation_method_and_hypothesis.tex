\section{Imputation Methods, Design and Hypothesis}
\label{sec:imputation_method}

% Complete case and other scenarios
For the analysis on sensitivity of \ac{OLS} results to different imputation method we compare the complete-case analysis to four imputation methods. For the complete-case analysis, we drop all observations which have missing values. For the analysis on the imputed data, we impute the missing values in the outcomes and covariates with the \ac{kNN} method or with a random draw from a normal distribution or simply the minimum or maximum of the respective variable. In the following, we first introduce these two methods in more detail and then present the different imputations, conducted for our sensitivity analysis the \ac{OLS} results. in the last paragraph on this section we state our expected hypothesis for the conducted imputation designs.

\subsection{Imputation methods}
% kNN
The \ac{kNN} imputation method is completing missing values using the observation which has the smallest distance to itself. Thereby, the Euclidean distance is used to specify the nearest observation. If the nearest neighbor is specified and has as well a missing for the same value, then the second nearest neighbor is taken. Hence, if an observation has more than one missing, the nearest neighbor for each missing might be different. In case all observations are missing for the observations, the average of the respective sample is taken. If one observation has the same distance to all of the remaining observations, the next observations by order is taken. To account for the difference scales of the variables, the variables are normalized. After completing the missings, the values are converted back to their original scales. In this analysis, we divide in the \ac{kNN} imputation method between the treatment and control group, i.e., the missings in the treatment group are filled by neighbors from the treatment group and the missings in the control group are filled by the neighbors in the control group.

The function for the imputation uses the package textit{StandardScaler} from the \textit{sklearn.preprocessing}, to normalize the varaibels and \textit{KNNImputer} package from the \textit{sklearn.impute}, to complete the missing with the nearest neighbor.

% MSD
The second method is completing the missings by drawing from a normal distribution whereby the mean equals the median of the respective variable/ column and the standard deviation equals a quarter of the standard deviation of each column respectively. The number of draws can be chosen. Then the mean of the random draws are imputed. For our analysis, we use only one draw. As in the \ac{kNN} imputation, we divide between the treatment and control groups. The missings are completed by the normal distribution defined by the variable median and one forth of the standard deviation of the respective variable in the respective group.

% MIN MAX
The remaining imputation methods, complete the missings by simply imputing the minimum or maximum of a variable, respectively. Again, as explained above, for the imputation we differentiate between the treatment and control group. Missings in both groups are imputed separately, based on the minimum and maximum of the respective variable, in the respective sample group.

\subsection{Imputation design}
% SCENARIOS
Based on the imputation methods explained above, we design four scenarios, the way we impute data, additionally to the complete-case analysis. The imputation of the missings in the outcomes and covariates are conducted as follows.

\begin{itemize}
	\item[1.] In the fist scenario, we impute the missings in the outcomes and covariates with the \ac{kNN} imputation method.
	\item[2.] In the second scenario, we impute the missings in the outcomes with a random draw from a normal distribution, as explained above in the first step. In the second step, we impute the missings in the covariates with the \ac{kNN} method.
	\item[3.] In the third scenario, we impute the missings in the outcomes with the minimum, as explained above in the first step. In the second step, we impute the missings in the covariates with the \ac{kNN} method.
	\item[4.] In the fourth scenario, we impute the missings in the outcomes with the maximum, as explained above in the first step. In the second step, we impute the missings in the covariates with the \ac{kNN} method.
\end{itemize}

From now on, we refer to these imputation scenarios as, ``kNN scenario'', ``kNN-msd scenario'', ``kNN-min scenario'' and ``kNN-max scenario'', respectively.

\subsection{Hypothesis}
% HYPOTHESIS
For the four different imputation designs, we established three main hypothesis:

\begin{itemize}

	\item[H1] For the ``kNN'' case we hypothesize that there will not be a notable change in the treatment coefficient, ``Treatment'' and the standard deviation for all estimates decreases. The first part of the hypothesis is based on the analysis on the missings. The analysis on the missings has shown that the missings are possibly generated based on the \ac{MAR}, i.e., they correlate with observed characteristics. Hence, imputing the missings based on the observed information should not change the estimation on the treatment. We expect the standard error to decrease because we impute from the same available sample and hence distribution and because the sample size increases with imputation.

	\item[H2] For the ``kNN-msd scenario'' we hypothesize that the treatment effect, ``Treatment'' decreases and that standard errors of the estimates decrease. We expect the treatment effect to decrease because most of the missings are observed in the control group and for people with lower income levels see section 3.2, Table~\ref{tab:table_missing}. Through the imputation of the outcome variable, household income in the second wave, with the median plus, minus some noise, leads to possibly better values for the ones who dropped out or did not provide information on the outcome. This leads to the fact that the imputation decreases the difference between the treatment and control group on the outcome variable. Hence, the results show a lower effect of the treatment. As above, the standard errors decrease because the missings are imputed from the same sample and the number of observations increases in the

	\item[H3] For the ``kNN-min scenario'' and the ``kNN-max scenario'', we hypothesize that the treatment effect will decrease when imputing the maximum and increase when imputing the minimum while the standard deviation of all estimates tends to increase. The ``kNN-min scenario'' will hence provide the upper bound while the ``kNN-max scenario'' will provide the lower bound of the treatment effect estimate. As mentioned above, the first part of the hypothesis relies of the fact that missings are mostly observed for people in the control group. When completing the missings in the outcome with the maximum value, the distance between the treatment and control group decrease and the positive treatment effect observed decreases. Contrary, to this, completing the missings in the outcome with the minimum increases the difference between the treatment and control group in the outcome, enforcing the positive treatment effect. The standard errors are expect to increase since the imputation is based on the two most extreme values of the distribution. Finally, all the estimates produced for the treatment effect should fall into the bounds of the ``kNN-min scenario'' and the ``kNN-max scenario''.

\end{itemize}

We compare our hypothesis with the results in section 5.


